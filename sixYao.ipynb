{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "import argparse\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import random\n",
    "import scipy.stats as sc\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = [\"生活不易，要好好珍惜。\",\n",
    "            \"学无止境，要不断进步。\",\n",
    "            \"人生短暂，要活得有意义。\",\n",
    "            \"爱与被爱，是人类最美好的情感。\",\n",
    "            \"勤奋和毅力，是成功的关键。\",\n",
    "            \"没有付出就没有收获。\",\n",
    "            \"健康是最大的财富。\",\n",
    "            \"时光荏苒，珍惜眼前人。\",\n",
    "            \"未来可期，要有信心。\",\n",
    "            \"诚信为本，处事公正。\",\n",
    "            \"人与人之间要互相尊重。\",\n",
    "            \"团结就是力量。\",\n",
    "            \"金钱不能代替一切。\",\n",
    "            \"知足常乐，心态很重要。\",\n",
    "            \"人生有苦有乐，要看淡一些。\",\n",
    "            \"热爱生活，享受每一天。\",\n",
    "            \"科技在不断进步，我们也要不断学习。\",\n",
    "            \"幸福就在身边，要学会发现。\",\n",
    "            \"人生就像一场旅行，要不断探索。\",\n",
    "            \"梦想不只是梦，要用行动去实现。\",\n",
    "            \"生命不息，奋斗不止。\",\n",
    "            \"不忘初心，方得始终。\",\n",
    "            \"世上无难事，只怕有心人。\",\n",
    "            \"好的开始是成功的一半。\",\n",
    "            \"沉默不是金，适时表达很重要。\",\n",
    "            \"勇敢面对困难，你会更强大。\",\n",
    "            \"机会只留给有准备的人。\",\n",
    "            \"爱情需要用心经营。\",\n",
    "            \"有志者事竟成。\",\n",
    "            \"做事要有计划和步骤。\",\n",
    "            \"改变自己，改变世界。\",\n",
    "            \"坚持就是胜利。\",\n",
    "            \"珍惜友情，朋友很重要。\",\n",
    "            \"年轻就是资本，要好好利用。\",\n",
    "            \"世界上没有偶然，只有必然。\",\n",
    "            \"勇于创新，不断超越自己。\",\n",
    "            \"现在努力，未来才会更美好。\",\n",
    "            \"难以忘怀的经历，是人生中的财富。\",\n",
    "            \"宽容和理解，让世界更美好。\",\n",
    "            \"热爱生命，就会更有活力。\",\n",
    "            \"坚定信念，无所不能。\",\n",
    "            \"路漫漫其修远兮，吾将上下而求索。\",\n",
    "            \"付出不一定有回报，但不付出一定无回报。\",\n",
    "            \"人生没有彩排，每一天都是现场直播\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 768])\n"
     ]
    }
   ],
   "source": [
    "modelname = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "bertmodel = BertModel.from_pretrained(modelname)\n",
    "input = tokenizer(train_set, padding=True,\n",
    "            return_tensors=\"pt\", return_attention_mask=True)\n",
    "output = bertmodel(**input)\n",
    "train_data = output[\"last_hidden_state\"].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if gpu else \"cpu\")\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(768, 512)\t\n",
    "        self.fc2 = nn.Linear(512, 256)\t\n",
    "        self.fc31 = nn.Linear(256, 3)\n",
    "        self.fc32 = nn.Linear(256, 3)\n",
    "        self.fc3 = nn.Linear(3, 256)\n",
    "        self.fc4 = nn.Linear(256, 512)\n",
    "        self.fc5 = nn.Linear(512, 768)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        h1 = F.relu(self.fc2(x))\n",
    "        return self.fc31(h1), self.fc32(h1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        dis = torch.distributions.Normal(mu, torch.exp(0.5*logvar))\n",
    "        return dis.rsample()\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc3(z))\n",
    "        h3 = F.relu(self.fc4(z))\n",
    "        return torch.sigmoid(self.fc5(h3))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 768))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n",
    "model = VAE()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 768), reduction='sum')\n",
    "\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "    return BCE + KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  5864.63623046875\n",
      "loss :  -26096.232421875\n",
      "loss :  -848900.9375\n",
      "loss :  -167888.484375\n"
     ]
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.DataLoader(train_data.detach(),batch_size=11, shuffle=True)\n",
    "for data in train_dataset:\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    recon_batch, mu, logvar = model(data)\n",
    "    loss = loss_function(recon_batch, data, mu, logvar)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"loss : \", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SIXYAO():\n",
    "    def __init__(self,model) -> None:\n",
    "        self.GUA = {0:\"阳\",1:\"阴\" ,2:\"阳\" ,3:\"阴\"}\n",
    "        self.BIANGUA = {0:\"阴\",1:\"阴\" ,2:\"阳\" ,3:\"阳\"}\n",
    "        self.BAGUA = {\"阴阴阴\":\"坤\",\"阴阴阳\":\"艮\",\"阴阳阴\":\"坎\",\"阴阳阳\":\"巽\",\"阳阴阴\":\"震\",\"阳阴阳\":\"离\",\"阳阳阴\":\"兑\",\"阳阳阳\":\"乾\"}\n",
    "        self.ben = None\n",
    "        self.bian = None\n",
    "        self.model = model\n",
    "\n",
    "    def bu(self, string):\n",
    "        modelname = 'bert-base-uncased'\n",
    "        tokenizer = BertTokenizerFast.from_pretrained(modelname)\n",
    "        bertmodel = BertModel.from_pretrained(modelname)\n",
    "        input = tokenizer(string, padding=True,\n",
    "                    return_tensors=\"pt\", return_attention_mask=True)\n",
    "        output = bertmodel(**input)\n",
    "        data = output[\"last_hidden_state\"].sum(axis=1)\n",
    "        collection = []\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.model.encode(data)\n",
    "            dis = sc.norm(mu, torch.exp(0.5*logvar))\n",
    "            z = self.model.reparameterize(mu, logvar)\n",
    "        coins  = torch.distributions.Bernoulli(torch.tensor(dis.cdf(z)))\n",
    "        for i in range(6):\n",
    "            coin = coins.sample().squeeze().numpy()\n",
    "            collection.append(coin)\n",
    "        collection = np.sum(collection, axis=1)\n",
    "        self.ben = [GUA[i] for i in collection]\n",
    "        self.bian = [BIANGUA[i] for i in collection]\n",
    "\n",
    "    def __solver(self, input):\n",
    "        output = []\n",
    "        for i in range(4):\n",
    "            index = \"\"\n",
    "            for j in input[i:i+3]:\n",
    "                index += j\n",
    "            output.append(self.BAGUA[index])\n",
    "        return output\n",
    "\n",
    "    def jie(self):\n",
    "        assert type(self.ben) != type(None), print(\"你需要先补一卦\")\n",
    "        print(\"本卦 :\", self.__solver(self.ben))\n",
    "        print(\"变卦 :\", self.__solver(self.bian))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "本卦 : ['乾', '兑', '震', '坤']\n",
      "变卦 : ['乾', '乾', '兑', '震']\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(time.time())\n",
    "sixyao = SIXYAO(model)\n",
    "sixyao.bu(\"赛博占卜\")\n",
    "sixyao.jie()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
